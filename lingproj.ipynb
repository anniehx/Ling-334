{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import gensim\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m spacy download en_core_web_lg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "love_is_blind = []\n",
    "for i in range(1,14):\n",
    "    if i < 10:\n",
    "        i = pd.read_csv(\"loveisblind4/Love.Is.Blind.S04E0\"+str(i)+\".WEBRip.NF.en.srt.csv\").text\n",
    "    else:\n",
    "        i = pd.read_csv(\"loveisblind4/Love.Is.Blind.S04E\"+str(i)+\".WEBRip.NF.en.srt.csv\").text\n",
    "    love_is_blind.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15938, 1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lib = pd.DataFrame(pd.concat(love_is_blind, axis=0, ignore_index=True))\n",
    "lib.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "love_island = []\n",
    "for i in range(1,30):\n",
    "    if i < 10:\n",
    "        i = pd.read_csv(\"loveislandaus4/Love.Island.Australia.S04E0\"+str(i)+\".WEBRip.NF.en.srt.csv\").text\n",
    "    else:\n",
    "        i = pd.read_csv(\"loveislandaus4/Love.Island.Australia.S04E\"+str(i)+\".WEBRip.NF.en.srt.csv\").text\n",
    "    love_island.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30984, 1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lisl = pd.DataFrame(pd.concat(love_island, axis=0, ignore_index=True))\n",
    "lisl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "too_hot_to_handle = []\n",
    "for i in range(1,9):\n",
    "    i = pd.read_csv(\"toohot1/Too.Hot.to.Handle.S01E0\"+str(i)+\".NF.WEBRip-HI.srt.csv\").text\n",
    "    too_hot_to_handle.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6681, 1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hot = pd.DataFrame(pd.concat(too_hot_to_handle, axis=0, ignore_index=True))\n",
    "hot.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function | clean symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def clean_text(subtitle_lst):\n",
    "    \n",
    "  cleaned_text = []\n",
    "\n",
    "  for subtitle_str in subtitle_lst:\n",
    "\n",
    "    # clean symbols\n",
    "    cleanhtml = re.compile('<.*?>')\n",
    "    clean1 = re.sub(cleanhtml, '', subtitle_str)\n",
    "\n",
    "    cleanbrackets = re.compile('\\[.*?\\]')\n",
    "    clean2 = re.sub(cleanbrackets, '', clean1)\n",
    "\n",
    "    cleancurly = re.compile('{.*?}')\n",
    "    clean3 = re.sub(cleancurly, '', clean2)\n",
    "\n",
    "    cleandash = re.compile('-')\n",
    "    clean4 = re.sub(cleandash, '', clean3)\n",
    "\n",
    "    cleanellipse = re.compile('\\…')\n",
    "    clean5 = re.sub(cleanellipse, '', clean4)\n",
    "\n",
    "    if \"\\n\" in clean5:\n",
    "      clean5 = clean5.replace(\"\\n\",\" \")\n",
    "\n",
    "    if \"\\xa0\" in clean5:\n",
    "      clean5 = clean5.replace(\"\\xa0\",\" \")\n",
    "\n",
    "    clean5 = clean5.lower() # lowercase \n",
    "\n",
    "    clean5 = clean5.lstrip().strip() # strip leading and ending spaces\n",
    "    \n",
    "    cleaned_text.append(clean5)\n",
    "\n",
    "  cleaned_text = list(filter(None, cleaned_text)) # remove empty strings\n",
    "  cleaned_text = [string for string in cleaned_text if string[0] != \"♪\"] # remove music subtitles\n",
    "\n",
    "\n",
    "  return cleaned_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function | tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(sub_lst):\n",
    "    \n",
    "    cleaned_lst = []\n",
    "\n",
    "    for subtitle in sub_lst:\n",
    "        \n",
    "        # replace names\n",
    "        for token in nlp(subtitle).ents:\n",
    "             if token.label_ == \"PERSON\":\n",
    "                subtitle = token.text.replace(token.text, \"PERSON\")\n",
    "        \n",
    "        doc = nlp(subtitle)\n",
    "\n",
    "        # remove punctuation, stop words\n",
    "        doc_cleaned_one = [token for token in doc if token.is_punct == False and token.text != ' ' and token.is_stop == False]\n",
    "        # remove additional words\n",
    "        remove_words = [\"gon\", \"na\", \"um\", \"wanna\", \"mmhmm\", \"oh\", \"uh\", \"ooh\", \"y'\", \"'em\", \"ta\", \"wo\", \"ya\", # filler sounds\n",
    "                        \"yes\", \"yeah\", \"okay\", \"hi\",  \"hello\", \"right\", \"hey\", \"guys\", \"lot\", \"like\", \"bit\", \"'cause\", # filler words\n",
    "                        \"know\", \"feel\", \"let\", \"think\", \"love\", \"good\", \"want\", \"going\", \"got\", \"mean\", \"said\", # too common\n",
    "                        # \"fuck\", \"shit\", \"girl\", \"girls\", \"fuck\", \"guy\", \"man\", \"bro\", #too common, don't use for love is blind\n",
    "                        \"micah\", \"amber\", \"tiffany\", \"PERSON\", \"zack\", \"stella\", \"irina\", \"chelsea\", \"irina\", \"shelby\",\"marshall\", # names\n",
    "                        \"rhonda\", \"francesca\", \"yoni\", \"sharron\", \"lana\", \"fran\", \"connor\", \"austen\", \"jordan\", \"maddy\", \"callum\", # names\n",
    "                        \"mitch\", \"phoebes\", \"majorca\", \"god\", \"browns\", \"phoebe\", \"yonis\" # names\n",
    "                        \"arizona\", \"mexico\" # places\n",
    "                        ]\n",
    "        \n",
    "        doc_cleaned_two = [token for token in doc_cleaned_one if str(token) not in remove_words]\n",
    "\n",
    "        cleaned_lst.append(doc_cleaned_two)\n",
    "\n",
    "    cleaned_lst = [x for x in cleaned_lst if x]\n",
    "\n",
    "    return cleaned_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_docs(show_subtitles):\n",
    "    docs = tokenize_text(clean_text(show_subtitles.text))\n",
    "    docs_str = []\n",
    "    for doc in docs:\n",
    "        docs_str.append([str(token) for token in doc])\n",
    "    docs = docs_str\n",
    "    return docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clean subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_docs = make_docs(lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11061"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lib_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lisl_docs = make_docs(lisl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22593"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lisl_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_docs = make_docs(hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4555"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hot_docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function | combine docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(docs, group_size):\n",
    "    combined_docs = []\n",
    "    num_groups = len(docs) // group_size\n",
    "\n",
    "    for i in range(num_groups):\n",
    "        docs_big = docs[i*group_size : (i+1)*group_size]\n",
    "        combined_docs.append([word for doc in docs_big for word in doc])\n",
    "\n",
    "    remaining_docs = docs[num_groups*group_size:]\n",
    "    if remaining_docs:\n",
    "        combined_docs.append([word for doc in remaining_docs for word in doc])\n",
    "    \n",
    "    return combined_docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# love is blind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_size = 5\n",
    "num_topics = 4\n",
    "\n",
    "lib_combined = combine(lib_docs, group_size)\n",
    "\n",
    "dictionary = gensim.corpora.dictionary.Dictionary(lib_combined)\n",
    "dictionary.filter_extremes(no_below=3)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in lib_combined] \n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word = dictionary, random_state = 1)\n",
    "\n",
    "%matplotlib inline\n",
    "vis = pyLDAvis.gensim.prepare(topic_model=ldamodel, corpus=corpus, dictionary=dictionary)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.save_html(vis, 'love_is_blind.html')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# love island "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_size = 5\n",
    "num_topics = 3\n",
    "\n",
    "lisl_combined = combine(lisl_docs, group_size)\n",
    "\n",
    "dictionary = gensim.corpora.dictionary.Dictionary(lisl_combined)\n",
    "dictionary.filter_extremes(no_below=3)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in lisl_combined] \n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word = dictionary, random_state = 1)\n",
    "\n",
    "%matplotlib inline\n",
    "vis = pyLDAvis.gensim.prepare(topic_model=ldamodel, corpus=corpus, dictionary=dictionary)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.save_html(vis, 'love_island.html')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# too hot to handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_size = 5\n",
    "num_topics = 3\n",
    "\n",
    "hot_combined = combine(hot_docs, group_size)\n",
    "\n",
    "dictionary = gensim.corpora.dictionary.Dictionary(hot_combined)\n",
    "dictionary.filter_extremes(no_below=2)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in hot_combined] \n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word = dictionary, random_state = 1)\n",
    "\n",
    "%matplotlib inline\n",
    "vis = pyLDAvis.gensim.prepare(topic_model=ldamodel, corpus=corpus, dictionary=dictionary)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.save_html(vis, 'too_hot_to_handle.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
